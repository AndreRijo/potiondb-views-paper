\documentclass{vldb}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{paralist}
\usepackage[inline]{enumitem}



\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)


\newcommand{\grumbler}[2]{{\color{red}{\bf #1:} #2}}
\newcommand{\andre}[1]{\grumbler{andre}{#1}}
\newcommand{\nuno}[1]{\grumbler{nuno}{#1}}
\newcommand{\carla}[1]{\grumbler{carla}{#1}}

\newcommand{\outline}[1]{}
%\newcommand{\outline}[1]{\grumbler{outline}{#1}}
\newcommand{\lineemph}[1]{\vspace{\baselineskip}\hspace{2em}\emph{#1}\vspace{\baselineskip}}

\vldbTitle{}
\vldbAuthors{}
\vldbVolume{12}
\vldbNumber{xxx}
\vldbYear{2020}
\vldbDOI{https://doi.org/TBD}


\begin{document}

\title{Global Views on Partially Geo-Replicated Data}
\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.

\author{\alignauthor André Rijo\\
       \affaddr{NOVA LINCS, FCT, Universidade NOVA de Lisboa}\\
%       \email{v.sousa@campus.fct.unl.pt}
\alignauthor Carla Ferreira\\
       \affaddr{NOVA LINCS, FCT, Universidade NOVA de Lisboa}\\
%       \email{carla.ferreira@fct.unl.pt}
\alignauthor
Nuno Preguiça\\
       \affaddr{NOVA LINCS, FCT, Universidade NOVA de Lisboa}\\
%       \email{nuno.preguica@fct.unl.pt}
}


\maketitle

\begin{abstract}
bla

bla

bla

bla

bla

bla

bla

bla

\end{abstract}

\section{Introduction}

%	\item Num. DC a aumentar

The increasing reliance on web service in many domains of activity, from e-commerce to business applications
and entertainment, leads to stringent requirements regarding latency, availability and fault tolerance \cite{Schurman2009latency,gomez}.
To address these requirements, cloud platforms have been adding new data centers at different geographic 
locations. By allowing users to access a service by contacting the closest data center, a global service can
provide low latency to users spread across the globe. The increasing number of data centers also contributes
for proving high availability and fault tolerance, by allowing a user to access the service by accessing any
available data center.

% serviços necessitam de data
% Replicar totalmente tem problemas

The database is a key component of any web service, storing the service's data. For supporting global
services running at multiple geographic locations, it is necessary to rely on a geo-replicated database \cite{dynamo},
which maintains replicas of the data at the data centers where the service is running.
A number of geo-replicated databases have been proposed, providing different consistency semantics.
Databases that provide strong consistency \cite{spanner,cockroachdb,mdcc} intend to give  the illusion that 
a single replica exists, requiring coordination among
multiple replicas for executing (update) operations. This leads to high latency and may compromise 
availability in the presence of network partitions.
Databases that provide weak consistency \cite{eventual,dynamo,cops} allow any replica to process a
client request, leading to lower latency and high availability. As a consequence, these databases expose
temporary state divergence to clients, making it more difficult to program a system. 

In either case, geo-replicated databases typically rely on a full replication model, where each data 
center replicates the full database, with data being sharded across multiple partitions in each data 
center. 
As both the data managed by these systems increases in size and the number of data centers increases,
this approach leads to a number of problems.
First, storing all data in all data centers imposes a large overhead in terms of storage. 
Furthermore, storing some data in all data centers may be unnecessary, as data is only needed at some
geographic locations.
Second, increasing the number of data centers makes the replications process more complex and costly, 
as each update needs to be propagated to all other data centers.

For addressing these problems, partial replication is an attractive approach, with each data center
replicating only a subset of the data. A number of works have been addressing the challenges of 
partial replication, for example by proposing algorithms to manage partially replicated data \cite{more,saturn,c3}
and to decide which data is replicated in which replica \cite{}.

In this paper we address the problem of querying data in a weakly consistent partially geo-replicated database, 
focusing on recurrent queries for which a programmer would want to generate a (materialized) view.
For example, consider an e-commerce system with users from multiple geographic locations.
In this case, the data pertaining users of a given location does not need to be replicated in all data centers
(but only in a few for fault tolerance). The same applies to other information, such as data on orders and 
warehouses.
Other data, such as information on products would be replicated in the regions where the product
is available.  
Under this data placement, obtaining the list of best seller products is challenging, as it requires
accessing data that is located at multiple data centers.

Several possible solutions exist for this problem. 
First, it is possible to have a data center that replicates all data, and forward these queries to such data center.
Doing this imposes a latency penalty and requires a data center to host all data and execute all queries of this type. %Andre: also refer that this data center would be the bottleneck of the system?
Second, it is possible to execute the query by accessing multiple locations, by using, for example, 
a distributed processing system with support for geo-partitioned data \cite{Kloudas:2015:POD:2850578.2850582,more}.
This approach requires running an additional external service and poses challenges for the consistency of the results
returned and the data observed by users.
%, mostly when adopting weak consistency models (a local update that should
%be reflected in the result of the query may not be returned, as the update might have not been read by the 
%external service that accessed a different replica).

We propose a different approach: to maintain materialized views, as commonly available in relational databases.
Implementing such feature efficiently in a partially geo-replicated database requires 
addressing two main challenges. 
First, it is necessary to guarantee consistency between the base data available in a replica and the 
relevant materialized views. To achieve this, we designed a replication mechanism where updates 
to the base data and views are made visible atomically in each replica.

Second, it is necessary to efficiently support views with limits, used for example to support \emph{top-k} 
queries. To achieve this, we build on the concept of non-uniform replication \cite{Cabrita17Nonuniform}, in which the state
of different replicas may be different, given that the observable state is (eventually) the same.
This allows each replica to propagate only the updates that might be relevant to the observable 
state. Providing support for views required us to extend non-uniform replication from simple data 
types to more complex structures that could support a view with multiple columns.
\nuno{can we support updates to the views? why not?}
\andre{short-answer: no. Since views don't have all data (e.g: only the name of a customer), we can't translate an update in a view to updates in other CRDTs, as such update would be incomplete}

We present the design and implementation of PotionDB, a geo-replicated key-value store with support  
for partial replication and materialized views. 
PotionDB provides weak consistency, for improved latency and availability, and support for highly
available transactions \cite{hat}.  
To our knowledge, our work is the first to address the problem of maintaining materialized views
in such setting.  

We have evaluated our system using micro-benchmarks and TPC-H queries \cite{}.
The results show that our algorithms for maintaining materialized views impose 
low overhead when executing and asynchronously replicating transactions, particularly
for views with limits.
\nuno{deviamos ter uns micro-benchmarks que comparassem o overhead com limites e sem limites}
Additionally, the results show that executing queries by relying on the materialized views is much more 
efficient than using alternative mechanisms.  
Furthermore, our algorithms for maintaining materialized views in a decentralized way perform better 
that alternative approaches where the view is computed in a single data center, while being
able to  keep consistency between the base and view data in every replica.

In this paper we make the following contributions:
%\begin{enumerate*}[label=(\roman*)]
\begin{itemize}
	\item the design of a geo-replicated key-value store  with support for partial replication
	and views over partially replicated data; 
	\item replication algorithms for efficiently maintaining consistent materialized views over 
	partially replicated data;
	 \item an implementation and evaluation of the proposed approach with micro-benchmarks
	 and TPC-H.
\end{itemize}

The remainder of the paper is organized as follows. Section ...

\outline{topicos

\begin{itemize}
	\item Num. DC a aumentar
	\item Replicar totalmente tem problemas
	\item Replicação parcial
	\item Queries sobre dados replicados parcialmente
	\begin{itemize}
		\item Standard solution?
	\end{itemize}
	\item Views materializadas replicadas totalmente
	\item Contribuições
\end{itemize}
}

\section{System overview}

\begin{itemize}
	\item System model
	\begin{itemize}
		\item Replicação parcial
	\end{itemize}
	\item System API
	\begin{itemize}
		\item "Create table"
		\item "Create view"
		\begin{itemize}
			\item CRDT não uniforme
			\item put numa table $\implies$ puts nas várias views
			\begin{itemize}
				\item consistência das views face aos dados - in sync
			\end{itemize}
		\end{itemize}
	\end{itemize}
	\item System description
	\begin{itemize}
		\item CRDT não uniforme
		\item Implementação de queries?
	\end{itemize}

\end{itemize}

\section{Implementation}

\section{Evaluation}

\section{Related Work}

\section{Conclusions}


\bibliographystyle{abbrv}
\bibliography{bib}

\section{System overview}

\vspace{8cm}
Possiveis pontos mais detalhados?

\subsection{System model}

\begin{itemize}
	\item Network assumptions
	\item Client-server interaction (refer key-value store interface? Maybe refer this instead in System API?)
	\item Server-server interaction? (is it needed? We'll already touch this in Replication.)
	\item System guarantees
	\begin{itemize}
		\item CRDTs
		\item Consistency level	
	\end{itemize}
	\item Replication
		\item Async
		\item Op-based
		\item Maintains consistency, i.e., transaction level based.
		\item Partial (system admin defined, each server only has a subset of the data based on topics. Potencially some data can be replicated everywhere)
\end{itemize}

\subsection{System API}

\begin{itemize}
	\item Basically how can we translate a problem to sql-like operations
	\item Create table
	\item Create view
	\item Updates (incluir problema de consistência de views/dados)
	\item Queries (incluir aqui problema de os CRDTs não uniformes precisarem de mais dados? Ou na zona da view?)
\end{itemize}

\subsection{System description}

\begin{itemize}
\item Structure? Maybe that's for implementation? How much detail?
\begin{itemize}
	\item Internal partitioning vs external partitioning? Capaz de não ser boa ideia...	
\end{itemize}
\item CRDTs and non-uniform CRDTs?
\end{itemize}

\andre{I ended up describing the topics of system description in other subsections, apart from Structure. I don't recall going into much detail of what a CRDT is, but that shouldn't be necessary anyway.}

\section{Implementation}

\begin{itemize}
	\item Go
	\item Transactions (TM/Mat?)
	\item Replication (RabbitMQ and other stuff?)
	\item Communication (protobufs. Also worth noticing the compability with existing AntidoteDB clients)
	\item CRDTs (version management at least)
\end{itemize}

\andre{A good part of the implementation is already included in other sections, at least indirectly. Mainly Replication and to an extent Transactions/Communication. We need to decide what really is important to refer in the "implementation" section.}

\null\newpage

\section{Motivating example}
\label{sec:example}

In order to both facilitate the understanding of the rest of the paper, as well as show the usefulness of queries supported by materialized views, we present the following example scenario.

Consider a large scale commerce company with stores and clients spread across the world.
Assume the company keeps data of millions of products, with each store having its own stock of products available.
Also consider that there are millions of both current and past customers and, among other data, all sales ever done are kept in the database, for both product warranty and statistical purposes.

Fully replicating the whole dataset would have proibitive costs.
It is also unecessary, as both the relevance of the data and likehood of being accessed from are dependent on the location.
For instance, an asian customer is more likely to consult the stock of stores in his country rather than in an european country.
Thus, it makes sence that both asian customers' and asian stores' data to be replicated mainly in asian datacenters (plus possibility a subset of others for fault tolerance purposes).
That is, the servers for which data will be replicated can be choosen based on the geographic location, as its relevance depends on such factor.

To keep the example simple, we will only consider three types of objects: customers, products and sales.
The simplified scheme of each object can be found on Figure \ref{fig:objects}.

\begin{figure}
	%\begin{table}[]
	\centering
	\begin{tabular}{|l|l|}
		\multicolumn{2}{c}{Customer} \\ \hline
		id            & int          \\ \hline
		name          & string       \\ \hline
		age           & int          \\ \hline
		country       & string      \\
		\hline
	\end{tabular} \hspace{0.7em}
	\raisebox{0.225\height}{\begin{tabular}{|l|l|}
			\multicolumn{2}{c}{Products} \\ \hline
			id           & int           \\ \hline
			name         & string        \\ \hline
			value        & int   \\
			\hline       
	\end{tabular}} \hspace{0.7em}
	\begin{tabular}{|l|l|}
		\multicolumn{2}{c}{Sales} \\ \hline
		id            & int       \\ \hline
		custID        & string    \\ \hline
		productID     & int       \\ \hline
		amount        & int	\\
		\hline      
	\end{tabular}
	\caption{Example objects}
	\label{fig:objects}
	%\end{table}
\end{figure}

Customers represent clients that at some point in time have bought at least one product from one of the stores.
Products represents items that may be (or have been) for sale.
Sales represents the aquisition of one or more units of a product by a client, where custID and productID refer to, respectivelly, the customer's and product's id field.

For the purpose of this example, we'll consider the following replication scheme for each type of object:
\begin{itemize}
	\item \emph{Customer:} replicated in the data centers present in the continent correspondent to his country (plus a few other data centers for fault-tolerance purposes);
	\item \emph{Products:} replicated in all datacenters;
	\item \emph{Sales:} replicated in the same data centers as of the customer who bought the product. 
\end{itemize}

Despite the fact that some objects aren't replicated everywhere, PotionDB will still support queries that refer to a global view of the database.
E.g., queries such as ``top 100 customers who have spent the most across all stores'' must be efficiently handled by PotionDB, even though no server contains all customer and sales data. 
This kind of queries allows to gather important real-time statistics which allow businesspersons to make decisions and changes on the business strategy to improve its rentability.
%Note that each object being partially replicated does not prevent queries that refer to data replicated in different datacenters from being executed.
%On section ??? we'll see how can this be done efficiently.

\andre{Should I refer tpc-h here? Do I need to motivate better the example/relevance of global queries*}

\section{System Overview}

PotionDB is a geo-distributed storage system which provides weak consistency or, more precisely, causal consistency.
A key feature in PotionDB is the ability to efficiently provide global views on partially replicated data under the conditions mentioned previously.
In this section we discuss design decisions which made it possible to provide such feature.

\subsection{System model}

We consider an asynchronous distributed system composed by a set of servers S connected by a network. 
We assume the network may delay, duplicate or re-order messages, but does not corrupt them. 
We also assume messages sent are eventually delivered, even if connections may temporarely drop.

Database objects are replicated in one or more servers, but not necessarely in all of them.  
We consider a server to be a replica of an object if it replicates such object. 
New updates are sent periodically to replicas asynchrnously. 
We don't require for all updates to be delivered to all replicas, as will be detailed in Sections \ref{subsubsec:replication} and \ref{subsubsec:nureplication}
%However, not all updates need to be delivered to every replica - only updates that modify the objects' visible state must be delivered eventually \ref{non-uniform}.

Clients connect to one or more servers. 
Updates and queries are executed in a single server and return as soon as its execution ends.
Clients may group multiple operations together in a single transaction.
Only objects replicated in the server may be accessed, i.e., to access objects not present in a given server, the client must connect to a replica of those objects.

\subsection{System API}

\subsubsection{Key-value store API}

PotionDB provides a key-value store interface \cite{???}.
%TODO: Proper formatting of GET/UPDATE/LINK structures/examples
%PotionDB is a key-value store database. %TODO: Reference?
As such, all objects are indexed by a key and support both \emph{get} and \emph{update} operations which, respectively, query/modify the state of an object.
In PotionDB objects are CRDTs \cite{???}, which ensures that even if objects are modified concurrently in different replicas, their states will eventually converge.
This allows for both \emph{get} and \emph{update} operations to be executed locally, with the effects of \emph{updates} being propagated asynchronously to other replicas.

To facilitate development of applications that want to use PotionDB, both \emph{gets} and \emph{updates} may refer to either the full object state or part of it.
A \emph{get} has the following interface: %\\

%Get CRDT key bucket crdtType\{arguments\}\\
\lineemph{Get CRDT key bucket crdtType\{arguments\}}

The triple key, bucket, crdtType are mandantory arguments used to uniquely identify an object
Bucket is used mainly for partial replication purposes, as will be described in Section \ref{subsubsec:replication}. 
Arguments are optional as they're used only for reading part of the state, with the possible arguments depending on the type of CRDT. 
E.g., for a map CRDT, we could use: \\

Get CRDT customer1 customers MAP\{name, age\}\\

%to return only the values in the customer1 map CRDT referred by the map keys name and age.
to return only the name and age of a customer stored as a map CRDT.

\emph{Updates} have a similar interface: \\

Update CRDT key bucket crdtType\{arguments\} \\

Key, bucket and crdtType have the same meaning as in \emph{get}.
The type of arguments depend on the type of CRDT, and each CRDT may support multiple types of update operations. 
E.g., a map supports both addition and removal of key/value pairs.
An insertion/update of an entry in a map can be represented as: \\

Update CRDT customer1 customers MAP\{ADD\{name: "David"\}\} \\

\subsubsection{API for view CRDTs}
\label{subsubsec:APIView}
%TODO: We ALMOST SURELY need a section before this explaining what is partial replication, its advantages/disadvantages, why we use it and why views are interesting for that.
%TODO: This may be more fit to another section, or split in multiple subsections in different sections of the paper

In relational databases (and even some non-relational ones, e.g., Cassandra) it's common to have \emph{Views}. 
A view can be defined as being the result of a query on one or multiple objects.
Views can then be accessed as if they were normal objects, thus facilitating the definition of new queries.
%TODO: Does this need a SQL example here?

Views can either be materialized or not (albeit not all databases support both). %TODO: references 
In a materialized view, the result of the query is stored as an object, which can then be reused later.
This can potencially improve drastically the performance of certain queries, but incours an extra cost on object updates, as both the object and the materialized view need to be updated.
On the other hand, non-materialized views don't have this extra cost, but they also don't usually improve the performance of queries, as their result is re-calculated on each query.

In PotionDB, it is particularly interesting to support materialized views.
To ilustrate that, recall the example tables and scenario defined in Section \ref{subsec:example}, along with the following query: 
%TODO: Do some proper emphasis on this
%TODO: Also consider doing this only for a continent instead of globally.

\emph{Determine the 100 customers who have spent the most across all stores. For each customer, the name, age, country and total value spent must be returned.}

Without a view, to implement this query it would be necessary to communicate with datacenters across the whole world (as sales and customers are partitioned). 
Since there are millions of customers and sales, we would not only need to download large amounts of data but also do time consuming data joins.
Only after calculating the total value spent for each customer in the service would we be able to reply with the top 100 customers.
Due to the sheer amount of data involved, this would have unnaceptable performance.

A possible solution is to use a Top-K CRDT %TODO: ref to NuCRDTs paper
as a materialized view which keeps the list of the 100 customers with highest (global) spendings.
This way, the query could be answered by executing only one \emph{get} operation on this CRDT.
This solution does pose some difficulties, namelly: 
%associate; keep entries updated even if the CRDT referred is not replicated locally; have new customers/sales be automatically included in the top-k.
\begin{enumerate*}[label=(\roman*)]
	\item association of each entry in the top-k with the respective customer and sales' CRDTs;
	\item translation of updates in customers or sales to updates for the top-k;
	\item keeping top-k updated even if some entries refer to CRDTs not replicated in the replica in which the top-k is created;
	\item automatically include customer and sales CRDTs that may be introduced after the creation of the top-k.
\end{enumerate*}

To support materialized views, we thus introduce another construct in our API, named \emph{link}.
The intuitive idea is that, after creating the CRDT that will be acting as a view, we can issue a \emph{link} operation in order to provide the "rules" for the automatic updating of the view CRDT.
PotionDB will then use these "rules" to keep the CRDT updated.

A link operation has the following structure:
\\

Link CRDT key bucket crdtType \\
From (key1*, bucket1*, crdtType1), ..., (keyN*, bucketN*, crdtTypeN) \\
Where conditions \\
Update updateArgs \\

%TODO: How do we tell the TopK crdt which fields are: a) used for uniquely identifying an entry, b) used for sorting. Maybe as argument in the type? Or a special create/empty update operation?

Similarly to \emph{gets} and \emph{updates}, each triple key, bucket, crdtType uniquely identifies an object.
The link operation contains four constructs:
%In short, \emph{link} specifies the CRDT that will be a view, while \emph{from} and \emph{where} specify which CRDTs will trigger updates on the view CRDT.
%Update specifies how to update the view CRDT. 
%More precisely:

\begin{itemize}
	\item \emph{Link}: specifies which CRDT will be acting as a view, i.e., the target of the \emph{Update} construct;
	\item \emph{From}: Similarly to the \emph{from} construct in SQL, it specifies which CRDT(s) will be used to build the view;
	In other words, whenever an update happens in one of these CRDTs, an update may also be triggered in the link CRDT.
	Note that in this construct exclusively, both key and bucket may be a prefix of the keys/buckets of the CRDTs (this is represented by the '*' character).
	This allows to include objects that may not yet exist in the database but that should also be included when they get added (e.g., new customers);
	\item \emph{Where}: Similary to the \emph{where} construct in SQL, it specifies using conditions how can the CRDTs returned in front be joined/filtered. E.g., if we only wanted sales related to portuguese customers: where customers.MAP{country} = "Portugal" \emph{and} sales.MAP{custID} = customers.MAP{id} \emph{and} sales.MAP{productID} = products.MAP{id};
	\item \emph{Update}: Specifies the operation that, for each entry returned by \emph{from} after applying \emph{where}, will be used to update the CRDT specified in \emph{link}.
\end{itemize}

%TODO: This almost surelly needs to be better explained here
The intuition is that when Link is first executed for a view CRDT, the view is updated based on the already existing objects that match the \emph{from} and \emph{where} clause.
Afterwards, whenever a CRDT that matches \emph{from} is updated, the \emph{where} clause is executed for that CRDT only and, if necessary, the view CRDT is updated.
On Section ??? we discuss in detail different ways to implement this, as well as how it is done in PotionDB. %TODO

%TODO: Proper linkage
As an example, this link operation generates a TopK CRDT which directly answers the query presented before:
\\
Link CRDT topsales statistics TOPK \\
From (customers*, customers, MAP), (products*, products, MAP), (sales*, sales, MAP) \\
Where sales.MAP\{customerID\} = customers.MAP\{id\} \emph{and}  sales.MAP\{productID\} = products.MAP\{id\} \\
Update MAP\{name: customers.MAP\{name\}, age: customers.MAP\{age\}, country: customers.MAP\{country\}, spent: products.MAP\{value\} * sales.MAP\{value\}\} \\

\subsection{System description}


\subsection{[PLACEHOLDER]Replication and Consistency}

PotionDB is a weakly consistent partially geo-replicated database.
Updates and queries are done locally in a replica, without needing to consult other replicas to reply to the clients' operations.
This implies that states between replicas may temporarly diverge, but eventually they need to converge.
Also, updates on data must be reflected on both the base data and their views atomically.

In this section we discuss multiple design decisions related with how consistency and replication is handled in PotionDB.

\subsubsection{Internal partitioning}

Internally in PotionDB, objects are automatically partitioned by computing an hash based on the object's key, bucket and type.
This should not be confused with partial replication, as both partitioning mechanisms have different goals.

The idea of internal partitioning in a replica comes from the observation that, by having data grouped in multiple "slots" (partitions), if different clients access objects in different partitions, then both requests can be processed concurrently by a replica without any conflict.
In the case of both requests being reads, they can be processed concurrently even if some objects are present in both requests. %TODO: This may need some extra explanation?

In theory this can lead to a considerate performance improvement, as each replica can make use of their multi-core CPUs by having one thread per partition to process requests.
Other possible solutions include: 
\begin{enumerate*}[label=(\roman*)] 
	\item \label{item:locks} using locks to protect the same object from being concurrently modified \cite{???};
	\item \label{item:single} using only a single thread \cite{???}.
\end{enumerate*}

Alternative \ref{item:locks} is difficult to implement, as it is needed to find the right level of lock granularity and when should locks be obtained/released \cite{???}. 
Special care is needed to avoid deadlocking concurrent transactions. 
There's also concerns with fairness of obtaining locks.
Our solution does not need to lock data, thus it avoids the overhead from using locks, it is easier to implement and does not have deadlocks.

As for \ref{item:single}, the solution does not take any benefict from the multiple cores of nowadays CPUs \cite{???}. In section \ref{???}, we show that this solution has worse performance when compared to ours, specially in read-only transactions.

%This likelly deserves a section of its own.
\subsubsection{Transactions and consistency}

Transactions allows to group multiple operations together and have them executed with certain guarantees.
They facilitate the usage of database systems \cite{???}.
For example, when registering a sale of a product, it is useful to have the product stock and customer info to be updated simultaneously. %TODO: We might need a better example

In the case of relational databases with strong consistency \cite{???} %TODO: Examples
, they usually provide ACID (atomicity, strong consistency, isolation, durability) properties for each of their transactions.
In short, they ensure that if a transaction completes sucessfully, it got executed: 
\begin{enumerate*}
	\item all of its operations completed successfully;
	\item the database is left in a consistent state;
	\item without interferance of other transactions;
	\item the effects of the operations won't be lost even if one of the replica fails.
\end{enumerate*}
One example in which all ACID properties are quite useful is when transfering money between two users. Isolation is needed to avoid two concurrent money withdraws (as the user may not have enough money for that), while atomicy, durability and consistency together prevent scenarios in which the sum of money between both users would get altered (which would be incorrect).

\andre{TODO: Definitely need to mention here that we can't have ACID in a distributed DB without having strong consistency. But I need papers to back this up.} %Group this with the paragraph below.

Even without all of ACID guarantees transactions are still useful, as is evidencied by multiple No-SQL databases which still provide some sort of transactions \cite{???} with fewer guarantees. %Cassandra, Redis, etc.

%TODO: I need to mention somewhere the benefict of read-only transactions (is there any actually?)
In PotionDB we also provide support for transactions. 
Transactions in PotionDB can span any number of partitions (both internal and external) existent in a replica, but it cannot refer to buckets that aren't replicated locally.
That is, transactions are local to a replica.
We support both read-only, write-only and mixed transactions, with all of them having the same guarantees.
%PotionDB transactions provide casual consistency, that is, a transaction sees the effects of all transactions that causally happened before it.

PotionDB's transactions are executed atomically and provide casual consistency. 
They also run isolated from other transactions executed in the same replica. 
However, transactions may concurrently modify the same object if they're executed in different replicas.
Durability isn't ensured, as replication is asynchronous and operations don't need to be written to disk for a transaction to commit. %TODO: We probably need to argue on why this is OK.
%PotionDB ensures that transactions are atomic and consistent, and also guarantees isolation from other transactions executing in the same replica (but not others).
%Durability isn't ensured though, as replication is asynchronous and operations don't need to be written to disk for a transaction to commit. 

\andre{We don't exactly guarantee atomicity - if the replica executing the transaction fails while applying updates, the DB will be left in an inconsistent state. Which isn't exactly relevant, since data would be lost in that case anyway... We do guarantee that, if the server doesn't fail, the client either sees the effect of the whole transaction or of none of its operations.}

\andre{Also... how much detail of the transaction execution mechanism should I provide here? Or just talking about the guarantees is enough?}

Each replica maintains a vector clock which summarizes the current DB state.
Each entry in the vector clock corresponds to the latest commit of each replica that is known locally.
When a transaction starts, a copy of this vector clock is associated to the transaction.
The entry correspondent to the local replica is processed differently though - for that entry, a unique, monotonically increasing timestamp is assigned to each transaction.

\andre{Where (and should I?) do I explain how the version management works? Or should we leave that for a short paper in another conference?}

Both updates and reads take the transactions' vector clock into account.
More precisely, reads execute on the state of the object correspondent to the transaction's vector clock (i.e., ignoring all update operations that may have happened afterwards).
Updates generate new states which are then marked with the transaction's timestamp, in order for reads to refer to the correct version.
A transaction is considered commited only after all operations have been applied. 
At this moment, if it isn't a read-only transaction, the replica's entry on the local vector clock is updated and the client is notified.

PotionDB's guarantees in regard to transactions are related with how the vector clock is handled. More precisely:

%E.g., in a read-only transaction, even if some update operation concurrently modifies objects referred by the read transaction, the results returned by reads will all be using the version specified by the transaction. That is, it is as if the update operation happened after the reads.

\paragraph{Atomicity}  A transaction is atomic because the replica's vector clock is only updated after all operations are successfully applied. 
If at least one fails, all updates are rendered innefective as we return the CRDTs to their previous state.
Previous reads on that transaction are irrelevant due to the transaction being considered as aborted.
Reads on other transactions don't see the effects of a transaction until it is commited, thus aborts don't pose a problem to them.

\paragraph{Consistency} As every transaction has a vector clock associated, we can thus totally order all transactions. 
A transaction is only executed if all transactions with a smaller vector clock have already been executed. 
We ensure locally generated transactions are executed by the order specified by the monotonicaly increasing local timestamp.
Whenever a CRDT with views associated is updated, those views are also updated in the same transaction with the same clock.
Thus, PotionDB respects causality when executing transactions and, as such, provides causal consistency.

\paragraph{Isolation} %During the execution of a transaction, reads and updates are all executed on top of the version specified by the transaction's vector clock.
%Inside each partition there is no concurrency and all of a transaction's operations are executed in a row
There is no concurrency inside each partition.
Reads are executed on top of the version specified by the transaction's vector clock. %TODO: Should I refer here that txn updates are considered for the read albeit they don't change the state yet?
Updates are only applied when commiting and are executed sequentially inside the partition.
Updates from other transactions are either executed before the first operation of the current transaction, or after the last.
Thus, the effects of each local transaction can be ordered sequentially, which implies isolation.

%TODO: Should I refer the 2 phase-commit? And the fact that commitTS != startTS?

\subsubsection{Views consistency}

Supporting materialized views in a database, as was hinted in Section \ref{subsubsec:APIView}, poses some challenges.
Namelly, it is required to keep the view and base data synchronized, that is, when one is updated the other must also be updated at the same point in time.
This becomes particularly challenging when we consider that views may refer to data that isn't replicated locally.

Ideally, updates to the view should be automatically generated based on updates to the base data and appended to the same transaction.
Nor only does this ensure the view and its base data are always in sync, it also avoids human errors and simplifies the client's code, as it doesn't need to be concerned with correctly updating the views.

\paragraph{Updating view with local data}
Recall the \emph{link} construct defined in Section \ref{subsubsec:APIView}.
The intuition here is that whenever an object is updated, all views associated to it should have their \emph{from} and \emph{where} clauses re-executed in order to generate the new set of updates that describe the new view state.
However, doing so would be inneficient, since it could imply accessing millions of objects uneccesarely just to update one entry.

\andre{How can this actually be implemented efficiently...?}

\paragraph{Updating view with remote data}

\andre{I can't explain much here until we decide on how exactly we want to cover views.}

\begin{itemize}
	\item Refer the main idea of our solution (and possibly discuss alternatives?)
	\item Require for view CRDTs to be replicated in all servers with base data for it
	\item Explain that allows for the view CRDTs updates to be generated
	\item Explain that since all replicas with a subset of the base data replicate the view CRDT, thus all of those will receive the view updates for all base data -> i.e., global view.
	\item Maybe include example of top-k?
\end{itemize}

\subsubsection{External partitioning}

Not all data is relevant everywhere. 
For example, in an e-commerce system, each customer's data is mostly relevant in the country/continent where they live.
PotionDB supports this by leveraging on partial replication, allowing the system admin to precisely define where each object is replicated.

Each object is identified by the triple key, bucket and crdtType.
Buckets define groups of objects that are replicated in the same group of replicas.
For each replica, the system admin can define which buckets will be replicated in it.
E.g., for partitioning customers based on their continent, we take the following steps:

\begin{enumerate}
	\item Define one bucket per continent;
	\item Configure the servers so that each replica replicates the bucket of its continent + one other for fault tolerance purposes;
	\item When adding the customers to the database, specify the correct bucket.
\end{enumerate}

PotionDB's replication mechanism takes in consideration the bucket distribution among the servers, ensuring only servers interested in a given bucket receive updates for that bucket.
Since data is partially replicated, the client must ensure that it is communicating with a server that replicates all buckets for which he wants to do operations upon, as all operations are executed locally.
That is, a replica can only receive operations for objects in buckets it replicates.

\subsubsection{Replication}
\label{subsubsec:replication}

Replication in PotionDB is asynchronous and partial.
Operations are executed localy, without needing to contact other replicas.
New updates are propagated in the background to other replicas periodically.
Objects in PotionDB are operation-based CRDTs, thus we replicate the arguments of operations instead of the new object state or a delta of it.

Each replica stores only a subset of all existing objects.
This subset is defined by a list of \emph{buckets}.
A bucket can be seen as a way to group objects together.
Each object in the database has both a key and a bucket associated to it, allowing to identify in which servers each object should be replicated to.

PotionDB uses RabbitMQ \cite{???} for handling communication between replicas.
RabbitMQ is a message brooker which allows consumers to register the topics of messages in which they're interested.
We leverage on topics to ensure each replica only fetches the messages containing updates for objects in buckets they are replicating.

We use RabbitMQ as follows.
When a replica starts, it subscribes to all messages whose topics match the buckets it replicates.
On the other hand, when a replica wants to publish new updates, it splits those updates by buckets (without breaking causality), ensuring each message only contains updates for one bucket and whose topic's value is that bucket.

\andre{I don't think the following paragraph is strictly necessary.}

Each PotionDB server runs alongside it a RabbitMQ instance.
Updates generated in that server are published to the local instance.
When the server is starting, it contacts the RabbitMQ instances of existing replicas and subscribes the buckets that server will be replicating.

We also use RabbitMQ to support the addition of new replicas without stopping the system.
However, such mechanism is out of scope of this paper and thus we don't describe it here.

\subsubsection{NuCRDTs replication}
\label{subsubsec:nureplication}

Non-uniform CRDTs \cite{???} leverage on the fact that, for certain objects, not all of the object's data is necessary to answer queries.
E.g., in a Top-K CRDT that only maintains the K elements with highest value, each replica only needs to maintain the elements that are in the top.

The key difference between eventual consistency and non-uniform eventual consistency is that, instead of requiring for the states of each object to be eventually equivalent, it requires for the \emph{observable} stables to be eventually equivalent \cite{???}.
Two states are defined as observable equivalent iff, for each possible query, the result is equivalent when executed on either states.
This allows to save both storage space and communication overhead, as not all updates in a non-uniform CRDT need to be replicated to all of its replicas \cite{???}.

Non-uniform CRDTs are specially useful for using as materialized views, as they allow to keep summaries of data easily while also being space-efficient.
E.g., in the scenario described in Section \ref{subsubsec:APIView}, even if there's millions of customers globally, replicas don't need to keep data for all customers in order to correctly apply reads in the Top-K CRDT.

\andre{What's above, technically, isn't 100\% true: each replica does need to keep ALL entries of the top-k due to removes being supported. Thus, an entry for all customers in the system. This WILL BE reflected in the experimental evaluation's graphics.}

\null\newpage\null

\null\newpage\null

\end{document}
